{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形モデル\n",
    "線形モデルによる二値分類  \n",
    "$d$次元ベクトルで表される事例$\\boldsymbol{x} \\in \\mathbb R^d$に対し、二値のクラス$\\hat{y} \\in \\{1, -1\\}$を下式で判別することを考える。\n",
    "$$\n",
    "\\hat{y} = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "        1 \\qquad (\\boldsymbol{w}^\\top \\boldsymbol{x} + b > 0) \\\\\n",
    "        -1 \\qquad (\\boldsymbol{w}^\\top \\boldsymbol{x} + b \\leqq 0)\n",
    "    \\end{array}\n",
    "\\right .\n",
    "$$\n",
    "ここで$\\boldsymbol{w}^\\top \\in \\mathbb R^d$、$b \\in \\mathbb R$はモデルのパラメータで、$\\boldsymbol{x} \\in \\mathbb R^d$である。\n",
    "これらはロジスティック回帰と同様の式だが、今後のことを考えて表記を$\\{1, 0\\}$から$\\{1, -1\\}$ととしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マージン最大化\n",
    "線形モデルのパラメータ$\\boldsymbol{w}、b$の値が決まると$\\hat{y}$を1, -1で分離する超平面を決めることができる。しかし、汎化性能を上げるにはデータ上にない点を考慮する必要がある。全学習事例と分離平面との距離を定量化するため全学習事例から分離面に推薦を下ろし、その距離を**マージン**とする。サポートベクタマシンではこれを最大にすることを考える。  \n",
    "今回の分離直線は$\\boldsymbol{w}、b$の二つのパラメータを持つが、マージンの計算すなわち$\\boldsymbol{w}、b$の計算に関与する事例を**サポートベクトル**という。  \n",
    "$y=1$のサポートベクトルを$\\boldsymbol{x}_+$、$y=-1$のサポートベクトルを$\\boldsymbol{x}_-$とし、$\\boldsymbol{w}$を分離平面に直行するもので初期化する。????????  \n",
    "マージンを$h$とすると、その大きさの二倍はベクトル$\\boldsymbol{x}_+ - \\boldsymbol{x}_-$を分離平面(今回の場合は分離直線)の法線ベクトル$\\boldsymbol{w}$に射影した大きさに等しいので、\n",
    "$$\n",
    "\\begin{align}\n",
    "2h &= \\frac{|(\\boldsymbol{x}_+ - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}}{\\|\\boldsymbol{w}\\|} \\notag  \\\\\n",
    "h &= \\frac{|(\\boldsymbol{x_+} - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}}{2\\|\\boldsymbol{w}\\|} \\qquad (*) \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "よってこれを最大化させることを考えるが、式からも分かるように$\\boldsymbol{w}$を定数倍してもマージン値は変わらない。つまり、現段階では$\\boldsymbol{w}$や$b$に関して任意性が残っている。??????????\n",
    "なので$\\boldsymbol{x}_+、\\boldsymbol{x}_-$に関して以下が成り立つとし、この任意性を排除する。\n",
    "<!-- $$\n",
    "\\left \\{\n",
    "    \\begin{align}\n",
    "    \\boldsymbol{w}^\\top \\boldsymbol{x}_+ + b &= 1 \n",
    "    \\boldsymbol{w}^\\top \\boldsymbol{x}_- + b &= -1 \\notag \\\\\n",
    "    \\end{align}\n",
    "\\right .\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "\\left \\{\n",
    "    \\begin{align}\n",
    "        \\begin{split}\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_+ + b &= 1  \\qquad \\tag*{$\\Leftrightarrow$}  \\\\\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_- + b &= -1 \\notag \\\\\n",
    "        \\end{split}\n",
    "    \\end{align}\n",
    "\\quad\n",
    "\\right \\{\n",
    "    \\begin{align}\n",
    "        \\begin{split}\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_+ + b - 1 &= 0 \\qquad \\tag{$**$} \\\\\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_- + b + 1 &= 0 \\notag \n",
    "        \\end{split}\n",
    "    \\end{align}\n",
    "$$\n",
    "これらより、$y_i=1$である事例は\n",
    "$$\n",
    "\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b \\geqq 1\n",
    "$$\n",
    "を満たし、$y_i = -1$は\n",
    "$$\n",
    "\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b \\leqq 1\n",
    "$$\n",
    "を満たす必要がある。まとめると、\n",
    "$$\n",
    "y_i (\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b) \\geqq 1\n",
    "$$\n",
    "また$(**)$を$(*)$に代入すると、\n",
    "$$\n",
    "\\begin{align}\n",
    "h &= \\frac{|(\\boldsymbol{x}_+ - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{|\\boldsymbol{x}_+^\\top \\boldsymbol{w} - \\boldsymbol{x}_-^\\top \\boldsymbol{w}|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{|(1 - b) - (-1 - b)|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{1}{\\|\\boldsymbol{w}\\|} \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "したがってサポートベクタマシンの学習は以下の制約付き最適化問題にて、パラメータ$\\boldsymbol{w}^*、b^*$を求める問題に帰着する。\n",
    "$$\n",
    "\\boldsymbol{w}^*, b^* = \\mathop{argmax}\\limits_{\\boldsymbol{w}, b} \\frac{1}{\\| \\boldsymbol{w} \\|}\\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots, N \\}: y_i (\\boldsymbol{w}^\\top \\boldsymbol{x} + b) - 1 \\geqq 0\n",
    "$$\n",
    "これを最小化問題に変更し、数学的に扱いやすくするために$\\frac{1}{2}$倍すると、SVMのための最小化問題は以下のようになる。\n",
    "$$\n",
    "\\boldsymbol{w}^*, b^* = \\mathop{argmin}\\limits_{\\boldsymbol{w}, b} \\frac{1}{2} \\| \\boldsymbol{w} \\| \\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots, N \\}: y_i (\\boldsymbol{w}^\\top \\boldsymbol{x} + b) - 1 \\geqq 0\n",
    "$$\n",
    "これらをラグランジュの未定乗数法で解く。\n",
    "$$\n",
    "L(\\boldsymbol{w}, b, \\boldsymbol{l}) = \\frac{1}{2} \\| \\boldsymbol{w} \\|^2 - \\sum^{N}_{i=1} \\lambda_i \\{y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b) - 1 \\}\n",
    "$$\n",
    "ラグランジュ関数$L$を最小にするため$\\boldsymbol{w}$で偏微分し、それを$0$と置くと、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{w}} = \\boldsymbol{w} - \\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i = 0 \\\\\n",
    "\\therefore \\boldsymbol{w} = \\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i\n",
    "$$\n",
    "$b$についても同様に、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\sum^{N}_{i=1} \\lambda_i y_i = 0 \\\\\n",
    "\\therefore \\sum^{N}_{i=1} \\lambda_i y_i = 0\n",
    "$$\n",
    "$L$の式をこれらの結果を利用して整理すると$\\boldsymbol{l}$に関する最小化問題を得る。この目的関数は元の目的関数の双対問題という。\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\boldsymbol{l}) &= \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1}\\lambda_j y_j \\boldsymbol{x}_j) - \\sum^{N}_{i=1} \\lambda_i [y_i \\{(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) \\boldsymbol{x}_i + b\\} - 1] \\notag \\\\\n",
    "&= \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) - (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) - b\\sum^{N}_{i=1} \\lambda_i y_i + \\sum^{N}_{i=1} \\lambda_i \\notag \\\\\n",
    "&= \\sum^{N}_{i=1} \\lambda_i - \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "よってSVMの学習の双対問題は以下のようになる。\n",
    "$$\n",
    "\\boldsymbol{l}^* = \\mathop{argmin}\\limits_{\\boldsymbol{l}} \\sum^{N}_{i=1} \\lambda_i - \\frac{1}{2} \\sum^{N}_{i=1}\\sum^{N}_{j=1} \\lambda_i \\lambda_j y_i y_j \\boldsymbol{x}_i^\\top \\boldsymbol{x}_j \\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots, N \\}: \\lambda_i \\geqq 0\\\\\n",
    "\\sum^{N}_{i=1} \\lambda_i y_i = 0\n",
    "$$\n",
    "### 二次計画法　SMO\n",
    "双対問題を最小にする$\\boldsymbol{l}^*$が求められたので$\\boldsymbol{w}^*$は、\n",
    "$$\n",
    "\\boldsymbol{w}^* = \\sum^{N}_{i=1} \\lambda_i^* y_i \\boldsymbol{x}_i\n",
    "$$\n",
    "またラグランジュの未定乗数法におけるKKT条件より、\n",
    "$$\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
