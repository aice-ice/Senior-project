{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形モデル\n",
    "線形モデルによる二値分類  \n",
    "$d$次元ベクトルで表される事例$\\boldsymbol{x} \\in \\mathbb R^d$に対し、二値のクラス$\\hat{y} \\in \\{1, -1\\}$を下式で判別することを考える。\n",
    "$$\n",
    "\\hat{y} = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "        1 \\qquad (\\boldsymbol{w}^\\top \\boldsymbol{x} + b > 0) \\\\\n",
    "        -1 \\qquad (\\boldsymbol{w}^\\top \\boldsymbol{x} + b \\leqq 0)\n",
    "    \\end{array}\n",
    "\\right .\n",
    "$$\n",
    "ここで$\\boldsymbol{w}^\\top \\in \\mathbb R^d$、$b \\in \\mathbb R$はモデルのパラメータで、$\\boldsymbol{x} \\in \\mathbb R^d$である。\n",
    "これらはロジスティック回帰と同様の式だが、今後のことを考えて表記を$\\{1, 0\\}$から$\\{1, -1\\}$ととしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マージン最大化\n",
    "線形モデルのパラメータ$\\boldsymbol{w}、b$の値が決まると$\\hat{y}$を1, -1で分離する超平面を決めることができる。しかし、汎化性能を上げるにはデータ上にない点を考慮する必要がある。全学習事例と分離平面との距離を定量化するため全学習事例から分離面に推薦を下ろし、その距離を**マージン**とする。サポートベクタマシンではこれを最大にすることを考える。  \n",
    "今回の分離直線は$\\boldsymbol{w}、b$の二つのパラメータを持つが、マージンの計算すなわち$\\boldsymbol{w}、b$の計算に関与する事例を**サポートベクトル**という。  \n",
    "$y=1$のサポートベクトルを$\\boldsymbol{x}_+$、$y=-1$のサポートベクトルを$\\boldsymbol{x}_-$とし、$\\boldsymbol{w}$を分離平面に直行するもので初期化する。????????  \n",
    "マージンを$h$とすると、その大きさの二倍はベクトル$\\boldsymbol{x}_+ - \\boldsymbol{x}_-$を分離平面(今回の場合は分離直線)の法線ベクトル$\\boldsymbol{w}$に射影した大きさに等しいので、\n",
    "$$\n",
    "\\begin{align}\n",
    "2h &= \\frac{|(\\boldsymbol{x}_+ - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}}{\\|\\boldsymbol{w}\\|} \\notag  \\\\\n",
    "h &= \\frac{|(\\boldsymbol{x_+} - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}}{2\\|\\boldsymbol{w}\\|} \\qquad (*) \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "よってこれを最大化させることを考えるが、式からも分かるように$\\boldsymbol{w}$を定数倍してもマージン値は変わらない。つまり、現段階では$\\boldsymbol{w}$や$b$に関して任意性が残っている。??????????\n",
    "なので$\\boldsymbol{x}_+、\\boldsymbol{x}_-$に関して以下が成り立つとし、この任意性を排除する。\n",
    "$$\n",
    "\\left \\{\n",
    "    \\begin{align}\n",
    "        \\begin{split}\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_+ + b &= 1  \\qquad \\tag*{$\\Leftrightarrow$}  \\\\\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_- + b &= -1 \\notag \\\\\n",
    "        \\end{split}\n",
    "    \\end{align}\n",
    "\\quad\n",
    "\\right \\{\n",
    "    \\begin{align}\n",
    "        \\begin{split}\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_+ + b - 1 &= 0 \\qquad \\tag{$**$} \\\\\n",
    "            \\boldsymbol{w}^\\top \\boldsymbol{x}_- + b + 1 &= 0 \\notag \n",
    "        \\end{split}\n",
    "    \\end{align}\n",
    "$$\n",
    "これらより、$y_i=1$である事例は\n",
    "$$\n",
    "\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b \\geqq 1\n",
    "$$\n",
    "を満たし、$y_i = -1$は\n",
    "$$\n",
    "\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b \\leqq 1\n",
    "$$\n",
    "を満たす必要がある。まとめると、\n",
    "$$\n",
    "y_i (\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b) \\geqq 1\n",
    "$$\n",
    "また$(**)$を$(*)$に代入すると、\n",
    "$$\n",
    "\\begin{align}\n",
    "h &= \\frac{|(\\boldsymbol{x}_+ - \\boldsymbol{x}_-)^\\top \\boldsymbol{w}|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{|\\boldsymbol{x}_+^\\top \\boldsymbol{w} - \\boldsymbol{x}_-^\\top \\boldsymbol{w}|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{|(1 - b) - (-1 - b)|}{2\\|\\boldsymbol{w}\\|} \\notag \\\\\n",
    "&= \\frac{1}{\\|\\boldsymbol{w}\\|} \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "したがってサポートベクタマシンの学習は以下の制約付き最適化問題にて、パラメータ$\\boldsymbol{w}^*、b^*$を求める問題に帰着する。\n",
    "$$\n",
    "\\boldsymbol{w}^*, b^* = \\mathop{argmax}\\limits_{\\boldsymbol{w}, b} \\frac{1}{\\| \\boldsymbol{w} \\|}\\\\\n",
    "s.t. {\\quad} ^\\forall\\!i \\in \\{1, \\cdots, N \\}: y_i (\\boldsymbol{w}^\\top \\boldsymbol{x} + b) - 1 \\geqq 0\n",
    "$$\n",
    "これを最小化問題に変更し、数学的に扱いやすくするために$\\frac{1}{2}$倍すると、SVMのための最小化問題は以下のようになる。\n",
    "$$\n",
    "\\boldsymbol{w}^*, b^* = \\mathop{argmin}\\limits_{\\boldsymbol{w}, b} \\frac{1}{2} \\| \\boldsymbol{w} \\| \\\\\n",
    "s.t.{\\quad} ^\\forall\\!i \\in \\{1, \\cdots, N \\}: y_i (\\boldsymbol{w}^\\top \\boldsymbol{x} + b) - 1 \\geqq 0\n",
    "$$\n",
    "これらをラグランジュの未定乗数法で解く。\n",
    "$$\n",
    "L(\\boldsymbol{w}, b, \\boldsymbol{l}) = \\frac{1}{2} \\| \\boldsymbol{w} \\|^2 - \\sum^{N}_{i=1} \\lambda_i \\{y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b) - 1 \\}\n",
    "$$\n",
    "ここで$\\pmb{l}$は、\n",
    "$$\n",
    "\\pmb{l} = ( \\lambda_1, \\cdots ,\\lambda_N)^\\top\n",
    "$$\n",
    "であり、$\\lambda_i(i=1, 2, \\cdots ,N)$はラグランジュ定数である。\n",
    "ラグランジュ関数$L$を最小にするため$\\boldsymbol{w}$で偏微分し、それを$0$と置くと、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{w}} = \\boldsymbol{w} - \\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i = 0 \\\\\n",
    "\\therefore \\boldsymbol{w} = \\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i\n",
    "$$\n",
    "$b$についても同様に、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\sum^{N}_{i=1} \\lambda_i y_i = 0 \\\\\n",
    "\\therefore \\sum^{N}_{i=1}   \\lambda_i y_i = 0\n",
    "$$\n",
    "$L$の式をこれらの結果を利用して整理すると$\\boldsymbol{l}$に関する最小化問題を得る。この目的関数は元の目的関数の双対問題という。\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\boldsymbol{l}) &= \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1}\\lambda_j y_j \\boldsymbol{x}_j) - \\sum^{N}_{i=1} \\lambda_i [y_i \\{(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) \\boldsymbol{x}_i + b\\} - 1] \\notag \\\\\n",
    "&= \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) - (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) - b\\sum^{N}_{i=1} \\lambda_i y_i + \\sum^{N}_{i=1} \\lambda_i \\notag \\\\\n",
    "&= \\sum^{N}_{i=1} \\lambda_i - \\frac{1}{2} (\\sum^{N}_{i=1} \\lambda_i y_i \\boldsymbol{x}_i)(\\sum^{N}_{j=1} \\lambda_j y_j \\boldsymbol{x}_j) \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "よってSVMの学習の双対問題は以下のようになる。\n",
    "$$\n",
    "\\boldsymbol{l}^* = \\mathop{argmin}\\limits_{\\boldsymbol{l}} \\sum^{N}_{i=1} \\lambda_i - \\frac{1}{2} \\sum^{N}_{i=1}\\sum^{N}_{j=1} \\lambda_i \\lambda_j y_i y_j \\boldsymbol{x}_i^\\top \\boldsymbol{x}_j \\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots, N \\}: \\lambda_i \\geqq 0\\\\\n",
    "\\sum^{N}_{i=1} \\lambda_i y_i = 0\n",
    "$$\n",
    "### 二次計画法　SMO\n",
    "双対問題を最小にする$\\boldsymbol{l}^*$が求められたので$\\boldsymbol{w}^*$は、\n",
    "$$\n",
    "\\boldsymbol{w}^* = \\sum^{N}_{i=1} \\lambda_i^* y_i \\boldsymbol{x}_i\n",
    "$$\n",
    "またラグランジュの未定乗数法におけるKKT条件より、\n",
    "$$\n",
    "\\left \\{\n",
    "    \\begin{align}\n",
    "        y_i(\\pmb{w}^\\top \\pmb{x}_i + b) - 1 \\geqq 0 \\notag \\\\\n",
    "        \\lambda_i \\geqq 0 \\notag \\\\\n",
    "        \\lambda_i \\{y_i(\\pmb{w}^\\top \\pmb{x}_i + b) - 1 \\} = 0 \\notag\n",
    "    \\end{align}\n",
    "\\right .\n",
    "$$\n",
    "3式目から$y_i(\\pmb{w}^\\top \\pmb{x}_i + b) > 1$のときの事例$\\pmb{x}_i, y_i$即ちサポートベクトルでない事例に対し手は$\\lambda_i = 0$が必ず成立する。ゆえに$\\pmb{w}^*$を求める式は学習中の少数のサポートベクトルの重み付き和で表現され、サポートベクトル以外の事例は計算に影響しない。  \n",
    "求めたサポートベクトルのいずれかに対して$(**)$を使用することで$b^*$を求める。\n",
    "$\\pmb{x}\\_$の場合\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pmb{w}^* \\pmb{x}\\_ + b^* &= -1 \\notag \\\\\n",
    "b^* = -\\pmb{w}^* \\pmb{x}\\_ - 1 \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "一般にサポートベクトルとなった事例のインデックスを$s$として特徴ベクトル$\\pmb{x}_s$、ラベル$y_s$とすると、\n",
    "$$\n",
    "b^* = y_s - \\pmb{w}^* \\pmb{x}_s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリで実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カーネルSVM\n",
    "入力$\\pmb{x}$に非線形変換を施し、非線形分類モデルを考える。$\\pmb{\\varphi}:\\mathbb{R}^d \\mapsto \\mathbb{R}^r$とし、モデルのパラメータは$\\pmb{w} \\in \\mathbb{R}^r$とバイアス$b \\in \\mathbb{R}$とするとき、線形の場合と同様に事例$\\pmb{x}$に対し分類器が予測するクラス$\\hat{y}$は、\n",
    "$$\n",
    "\\hat{y} = \\left \\{\n",
    "    \\begin{align}\n",
    "       \\begin{split}\n",
    "         +1 \\qquad (\\pmb{w}^\\top \\pmb{\\varphi}(\\pmb{x}) + b &> 0) \\qquad \\tag{$***$}\\\\\n",
    "         -1 \\qquad (\\pmb{w}^\\top \\pmb{\\varphi}(\\pmb{x}) + b &\\leqq 0) \\notag \n",
    "       \\end{split}\n",
    "    \\end{align}\n",
    "\\right .    \n",
    "$$\n",
    "これらの式は$\\pmb{x} \\to \\pmb{\\varphi}(\\pmb{x})$に置き換えているに過ぎないので、カーネルSVMにおける双代問題は\n",
    "$$\n",
    "\\pmb{l}^* = \\mathop{argmin}\\limits_{\\pmb{l}} \\sum^{N}_{i=1}\\lambda_i - \\frac{1}{2}\\sum^{N}_{i=1}\\sum^{N}_{j=1}\n",
    "\\lambda_i \\lambda_j y_i y_j \\pmb{\\varphi}(\\pmb{x}_i)^\\top \\pmb{\\varphi}(\\pmb{x}_j) \\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots ,N\\}: \\lambda_i \\geqq 0\\\\\n",
    "\\sum^{N}_{i=1}\\lambda_i y_i = 0\n",
    "$$\n",
    "$K(\\pmb{x}_i, \\pmb{x}_j) = \\pmb{\\varphi}(\\pmb{x}_i)^\\top \\pmb{\\varphi}(\\pmb{x}_j)$と置くと、双代問題は、\n",
    "$$\n",
    "\\pmb{l}^* = \\mathop{argmin}\\limits_{\\pmb{l}} \\sum^{N}_{i=1}\\lambda_i - \\frac{1}{2}\\sum^{N}_{i=1}\\sum^{N}_{j=1}\n",
    "\\lambda_i \\lambda_j y_i y_j K(\\pmb{x}_i, \\pmb{x}_j) \\\\\n",
    "s.t. ^\\forall\\!i \\in \\{1, \\cdots ,N\\}: \\lambda_i \\geqq 0\\\\\n",
    "\\sum^{N}_{i=1}\\lambda_i y_i = 0\n",
    "$$\n",
    "ここで$K$はカーネル関数と呼ばれ、サポートベクタマシンの学習では$\\pmb{\\varphi}(\\pmb{x})$を求めなくても、$K(\\pmb{x}_i, \\pmb{x}_j)$を全事例の組み合わせで求めれば十分である。また$\\pmb{\\varphi}$を計算せず$K$だけで計算を済ませる方法を**カーネルトリック**という。\n",
    "またカーネル関数には以下のようなものがある。\n",
    "- 線形カーネル\n",
    "$$\n",
    "K(\\pmb{u}, \\pmb{v}) = \\pmb{u}^\\top \\pmb{v}\n",
    "$$\n",
    "これに対応する$\\pmb{\\varphi}$は\n",
    "$$\n",
    "\\pmb{\\varphi}(\\pmb{x}) = \\pmb{x} \n",
    "$$\n",
    "よってこれは線形サポートベクタマシンそのもの\n",
    "- Radia Basis Function(RBF)\n",
    "$$\n",
    "K(\\pmb{u}, \\pmb{v}) = e^{- \\gamma \\|\\pmb{u} - \\pmb{v} \\|^2}\n",
    "$$\n",
    "これに対応する$\\pmb{\\varphi}$は無限次元になりうる\n",
    "- 多項式カーネル\n",
    "$$\n",
    "K(\\pmb{u}, \\pmb{v}) = (\\pmb{u}^\\top \\pmb{v} + c)^m\n",
    "$$\n",
    "$m=2$のとき\n",
    "$$\n",
    "\\begin{align}\n",
    "K(\\pmb{u}, \\pmb{v}) &= (\\pmb{u}^\\top \\pmb{v})^2 + 2c\\pmb{u}^\\top \\pmb{v} + c^2 \\notag \\\\\n",
    "&= \\sum^{N}_{i=1}u_i^2 v_i^2 + \\sum^{N}_{i=1} \\sum^{N}_{j=i+1} 2u_i u_j v_i v_j + \\sum^{N}_{i=1} 2c u_i v_i + c^2 \\notag \\\\\n",
    "&=  \\sum^{N}_{i=1} u_i^2 v_i^2 + \\sum^{N}_{i=1} \\sum^{N}_{j=i+1} (\\sqrt{2} u_i v_i)(\\sqrt{2} u_j v_j) + \\sum^{N}_{i=1} \\sqrt{2c} u_i v_i + c^2 \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "これに対応する$\\pmb{\\varphi}$は\n",
    "$$\n",
    "\\pmb{\\varphi}(\\pmb{x}) =  \n",
    "$$\n",
    "即ち$m=2$の時のカーネル関数は$\\pmb{x}$から各要素および各要素の2乗、2つの要素同士の積を$\\pmb{\\varphi}$で取り出すことと同値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カーネル関数を用いた分類\n",
    "カーネル関数を用いてサポートベクタマシンを学習することを考える。学習に必要なのは全事例の組み合わせ$i, j \\in \\{1, \\cdots , N \\}$に対する$K(\\pmb{x}_i, \\pmb{x}_j)$の値であるので$\\pmb{\\varphi}$の計算は不要である。\n",
    "$\\pmb{\\varphi}$に対する$\\pmb{w}^*$は\n",
    "$$\n",
    "\\pmb{w}^* = \\sum^{N}_{i=1} \\lambda_i^* y_i \\pmb{x}_i\n",
    "$$\n",
    "これを$(***)$に代入すると、\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pmb{w}^{*\\top} \\pmb{\\varphi}(\\pmb{x}) + b^* &= (\\sum^{N}_{i=1} \\lambda_i y_i \\pmb{\\varphi}(\\pmb{x}_i)) \\pmb{\\varphi}(\\pmb{x}) + b^* \\notag \\\\\n",
    "&= \\sum^{N}_{i=1} \\lambda_i^* y_i K(\\pmb{x}_i, \\pmb{x}) + b^* \\notag \n",
    "\\end{align}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
