{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lguq6LdkubjP"
      },
      "source": [
        "# 回帰\n",
        "### 回帰とは\n",
        "**回帰**とは 教師あり学習に分類される機械学習手法で、与えられたデータ(**説明変数**)から実数値を予測する手法である。またこの章で取り扱うのはその中でも説明変数が1次の**単回帰**と呼ばれるもので、説明変数が高次になると**重回帰**と呼ばれ、これは次章に取り扱う。単回帰では**回帰直線**を求め予測を行う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###　残差\n",
        "単回帰を実装するにあたって「残差」を導入する。\n",
        "今以下のような５つの事例を持つデータ$D$がある\n",
        "$$ \n",
        "D = \\{(x_{i}, y_{i})\\}_{i=1}^{N}= \\{(4, 15), (6, 8), (5, 17), (8, 15), (3, 1)\\} \n",
        "$$\n",
        "ここで１次関数$\\epsilon_{i}$モデル$\\hat{y} = ax + b$のa、bを$a = 2, b = 1$に設定して回帰直線を考える。\n",
        "また$\\hat{y}$は目的変数をモデルに適用し計算された予測値を表す。\n",
        "\n",
        "データ$D$の各点$(x_{i}, y_{i})$とモデルで計算される点$(x, \\hat{y})$を以下にプロットする\n",
        "<img src=\"material\\residuals\\plot1.png\">\n",
        "上図は１例であるが、a、bの値を変えることで様々な回帰直線ができる。しかし、設定したa、ｂがどのくらいデータにあっているかを定量的にみる必要がある。そこで使用するのが残差である。残差とは求めた予測値と実測値とのずれを表す数値で上図では各点と直線との距離に値する。よって残差の計算式は以下のようになる。(ここでは残差を$\\epsilon$で表し、データ$D$の$i$番目の要素に対応する残差を$\t\\epsilon_{i}$としている。)\n",
        "$$ \n",
        "\\epsilon_{i} = y_{i} - \\hat{y}_{i} \n",
        "$$\n",
        "したがって以下のような計算になる。\n",
        "$$\n",
        "\\left\\{ \n",
        "\\begin{array}{l}\n",
        "\\displaystyle  \\epsilon_{1} = y_{2} - \\hat{y}_{1} = 15 - 9 = 6\\\\\n",
        "\\epsilon_{2} = y_{2} - \\hat{y}_{2} = 8 - 13 = -5\\\\\n",
        "\\epsilon_{3} = y_{3} - \\hat{y}_{3} = 17 - 11 = 6\\\\ \n",
        "\\epsilon_{4} = y_{4} - \\hat{y}_{4} = 15 - 17 = -2\\\\ \n",
        "\\epsilon_{5} = y_{5} - \\hat{y}_{5} = 1 - 7 = -6\\\\ \n",
        "\\end{array} \n",
        "\\right .\n",
        "$$\n",
        "これで各事例の残差は計算できたのでデータ$D$の残差について考える。単純に各事例の残差を合計し、そこから平均をとっても、残差の正負で($\\epsilon_{1}$と$\\epsilon_{3}$のように)打ち消しあう可能性がある。また各事例の残差の絶対値をとってから平均を計算すると数学的な処理が困難になる。したがって2乗和の平均をとる**平均二乗誤差**を用いる。\n",
        "$$\n",
        " \\hat{L} = \\displaystyle \\frac{1}{N}\\sum_{n = 1}^{\\N}\\epsilon_{i} ^ 2 = \\displaystyle \\frac{1}{N}\\sum_{n = 1}^{\\N}(y_{i} - \\hat{y}_{i}) ^ 2\n",
        "$$\n",
        "データ$D$で計算すると\n",
        "$$\n",
        "\\hat{L} = \\displaystyle \\frac{1}{5}\\sum_{n = 1}^{5}\\epsilon_{i} ^ 2 = \\frac{1}{5}\\{6 ^ 2 + (-5) ^ 2 + 6 ^ 2 + (-2) ^ 2 + (-6) ^ 2\\} = 27.4\n",
        "$$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 最小二乗法\n",
        "前節で述べたようにa、bの値を変えることで様々な回帰直線を描くことができる。つまり平均二乗誤差はa、bの値によるのでデータに最適な回帰直線を得たいときは、平均二乗誤差を最小にするようなa、bの値を決める必要がある。また平均二乗誤差のようにモデルの出力値(予測値)と真の値(実測値)との誤差を表す関数を**損失関数**や**目的関数**というが、目的関数の方が広い意味で使用される。単回帰モデルの目的関数は平均二乗誤差の定義から以下のようになる。\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{L}(a, b) &= \\displaystyle \\frac{1}{N}\\sum_{n = 1}^{\\N}\\epsilon_{i} ^ 2 \\notag \\\\\n",
        "&= \\displaystyle \\frac{1}{N}\\sum_{n = 1}^{\\N}(y_{i} - \\hat{y}_{i}) ^ 2 \\notag \\\\\n",
        "&= \\displaystyle \\frac{1}{N}\\sum_{n = 1}^{\\N}\\{y_{i} - (ax_{i} + b)\\} ^ 2 \\notag \n",
        "\\end{align}\n",
        "$$\n",
        "このように誤差の2乗和を最小化してパラメータを求めることを**最小二乗法**という。\n",
        "ではこの目的関数を最小化することを考える。目的関数の式からも分かるようにa、bの２次関数で下に凸であるため、a、bについて微分しそれを0とおきそれを解くことになる。つまり下式を解くことになる。\n",
        "$$\n",
        "\\left \\{ \n",
        "\\begin{array}{l}\n",
        "\\displaystyle \\frac{\\partial \\hat{L}(a, b)}{\\partial a} = 0 \\\\\n",
        "\\displaystyle \\frac{\\partial \\hat{L}(a, b)}{\\partial b} = 0 \\\\\n",
        "\\end{array}\n",
        "\\right .\n",
        "$$\n",
        "まずbについて、\n",
        "$$\n",
        "\\displaystyle \\frac{\\partial \\hat{L}(a, b)}{\\partial b} = \\frac{2}{N}\\sum_{n=1}^{\\N}2 ・(y_{i} - ax_{i} - b)(-1) = 0 \\\\\n",
        "\\begin{align}\n",
        "\\displaystyle \\therefore  \\frac{1}{N}\\sum_{n=1}^{\\N}(y_{i} - ax_{i} - b) &= 0 \\notag \\\\\n",
        "\\displaystyle \\frac{1}{N}(\\sum_{n=1}^{\\N}y_{i} - a\\sum_{n=1}^{\\N}{x_{i}} - Nb) &= 0 \\notag  \\\\\n",
        "\\displaystyle b &= \\frac{1}{N}(\\sum_{n=1}^{\\N}y_{i} - a\\sum_{n=1}^{\\N}x_{i}) \\notag \\\\\n",
        "\\displaystyle b &= \\bar{y} - a\\bar{x} \\notag \n",
        "\\end{align} \n",
        "$$\n",
        "またaについても、\n",
        "$$\n",
        "\\frac{\\partial \\hat{L}(a, b)}{\\partial a} = \\frac{2}{N}\\sum_{n=1}^{\\N}2 ・(y_{i} - ax_{i} - b)(-x_{i}) = 0 \\\\\n",
        "\\begin{align}\n",
        "\\displaystyle \\therefore  \\frac{1}{N}\\sum_{n=1}^{\\N}x_{i}(y_{i} - ax_{i} - b) &= 0 \\notag \\\\\n",
        "\\displaystyle \\frac{1}{N}(\\sum_{n=1}^{\\N}x_{i}y_{i} -a\\sum_{n=1}^{\\N}x_{i} ^ 2 -b\\sum_{n=1}^{\\N}x_{i}) &= 0 \\notag \\\\\n",
        "\\displaystyle \\bar{xy} - a\\bar{x^2} - b\\bar{x} &= 0 \\notag \\\\\n",
        "\\displaystyle \\bar{xy} - a\\bar{x^2} - \\bar{x}(\\bar{y} - a\\bar{x}) &= 0 \\notag \\\\\n",
        "\\displaystyle a &= \\frac{\\bar{xy} - \\bar{x}\\bar{y}}{\\bar{x^2} - \\bar{x}^2} \\notag \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "よって単回帰でのパラメータを求める式は、\n",
        "$$\n",
        "a = \\frac{\\bar{xy} - \\bar{x}\\bar{y}}{\\bar{x^2} - \\bar{x}^2}, \n",
        "b = \\bar{y} - a\\bar{x} \n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 確認課題\n",
        "単回帰のパラメータを求める式を実装してください。その際scikit-learnなどのライブラリを使用せずに実装してください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
