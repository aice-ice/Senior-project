{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多クラス分類\n",
    "前章では２つのグループに分けることを考えたが、この章では３つ以上のグループに分けることを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベル推定式\n",
    "$d$次元の特徴ベクトル$\\boldsymbol{x}$が与えられているとき、分類の候補になる$K$個のクラスの集合$\\mathcal{Y}=\\{\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\cdots , \\mathcal{C}_{K}\\}$   wokangaeru \n",
    "\n",
    "線形多クラス分類では各カテゴリを$\\hat{y} \\in \\mathcal{Y}$に重みベクトル$\\boldsymbol{w}_{y} \\in \\mathbb R ^ d$を用意し、事例$\\boldsymbol{x} \\in \\mathbb R ^ d$と重みベクトル$\\boldsymbol{w}_{y}$との内積を計算し、最も高い内積値が計算されたカテゴリ$y \\in \\mathcal{Y}$に分類する。\n",
    "よって多クラス分類のラベル推定式は以下のようになる。\n",
    "$$\n",
    "\\hat{y} = \\argmax_{y \\in \\mathcal{Y}}\\boldsymbol{w}_y^\\top\\boldsymbol{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多クラスロジスティック回帰\n",
    "線形多クラス分類を実現するモデルの一つで事例$\\boldsymbol{x}$をクラス$\\mathcal{C}$に分類する条件付確率$P(\\hat{y}=\\mathcal{C}_j | \\boldsymbol{x})$を以下のように求める。\n",
    "$$\n",
    "P(\\hat{y}=\\mathcal{C}_j | \\boldsymbol{x}) = \\frac{e^{\\boldsymbol{w}_j^\\top\\boldsymbol{x}}}{\\displaystyle \\sum^{K}_{k=1}e^{\\boldsymbol{w}_k^\\top\\boldsymbol{x}}}\n",
    "$$\n",
    "$a_j = \\boldsymbol{w}_j^\\top \\boldsymbol{x}$と置くと上式は\n",
    "$$\n",
    "P(\\hat{y}=\\mathcal{C}_j | \\boldsymbol{x}) = \\frac{e^{a_j}}{\\displaystyle \\sum^{K}_{k=1}e^{a_k}}\n",
    "$$\n",
    "となり、この式はソフトマックス関数の形をしているため$\\sigma$をソフトマックス関数とすると以下のように置く。\n",
    "$$\n",
    "\\sigma(\\boldsymbol{a})_j = P(\\hat{y}=\\mathcal{C}_j|\\boldsymbol{x}) = \\frac{e^{a_j}}{\\displaystyle \\sum^{K}_{k=1}e^{a_k}} \n",
    "$$\n",
    "ここで、$\\boldsymbol{a}$は\n",
    "$$\n",
    "\\boldsymbol{a} = \\begin{pmatrix}\n",
    "\\boldsymbol{w}_1^\\top \\boldsymbol{x} \\\\ \\boldsymbol{w}_2^\\top \\boldsymbol{x} \\\\ \\boldsymbol{w}_3^\\top \\boldsymbol{x} \\\\ \\vdots \\\\ \\boldsymbol{w}_K^\\top \\boldsymbol{x} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "であり、$\\sigma(\\boldsymbol{a})_j$は$\\sigma(\\boldsymbol{a})$の出力の$j$番目の要素である。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力: [ 0.  0. nan]\n",
      "出力ベクトルの大きさ: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eu21052\\AppData\\Local\\Temp\\ipykernel_10332\\2871618204.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n",
      "C:\\Users\\eu21052\\AppData\\Local\\Temp\\ipykernel_10332\\2871618204.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "x = np.array([-5, 2, 1000])\n",
    "y = softmax(x)\n",
    "\n",
    "print(f'出力: {y}')\n",
    "print(f'出力ベクトルの大きさ: {np.sum(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の結果から分かるようにソフトマックス関数はシグモイド関数同様に出力を確率とみなすことができる。ただしシグモイド関数と異なる点は、ソフトマックス関数はクラス数を２以上に設定できる。したがってソフトマックス関数はシグモイド関数を拡張したものとみることができる。したがって２値分類ではシグモイド関数を、多クラス分類ではソフトマックス関数を使用する。\n",
    "しかし上で実装したコードでは以下のようなことが起こってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力: [ 0.  0. nan]\n",
      "出力ベクトルの大きさ: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eu21052\\AppData\\Local\\Temp\\ipykernel_10332\\2871618204.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n",
      "C:\\Users\\eu21052\\AppData\\Local\\Temp\\ipykernel_10332\\2871618204.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-5, 2, 1000])\n",
    "y = softmax(x)\n",
    "\n",
    "print(f'出力: {y}')\n",
    "print(f'出力ベクトルの大きさ: {np.sum(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のコードは入力ベクトルの要素の１つの値を1000に変えソフトマックス関数に入力している。しかし出力結果を見てもわかるようにソフトマックス関数の出力がオーバーフローしてしまっている。そこで今後使用するソフトマックス関数を以下のように変更する。\n",
    "$$\n",
    "\\sigma(\\boldsymbol{a})_j = \\frac{e^{(a_j - b)}}{\\displaystyle \\sum^{K}_{k=1}e^{(a_k - b)}}\n",
    "$$\n",
    "ここで$b$は任意の実数である。この$b$を適切に設定することでオーバーフローを防ぐ。\n",
    "またこの式は、\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma(\\boldsymbol{a})_j &= \\frac{e^{a_j}}{\\displaystyle \\sum^{K}_{k=1}e^{a_k}} \\notag \\\\\n",
    "&= \\frac{e^{(a_j - b + b)}}{\\displaystyle \\sum^{K}_{k=1}e^{(a_k - b + b)}} \\notag \\\\\n",
    "&= \\frac{e^be^{(a_j - b)}}{\\displaystyle e^b\\sum^{K}_{k=1}e^{(a_k - b)}} \\notag \\\\\n",
    "&= \\frac{e^{(a_j - b)}}{\\displaystyle \\sum^{K}_{k=1}e^{(a_k - b)}} \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "となるので変更後の式でも問題ないことがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最尤推定\n",
    "2値分類同様最尤推定を行っていくが大まかな流れは2値分類と同じである。学習事例$(\\boldsymbol{x}, \\boldsymbol{y})$に対するモデルパラメータ$\\boldsymbol{W}$の尤度$\\hat{l}_{\\boldsymbol{x},\\boldsymbol{y}}$は、\n",
    "$$\n",
    "\\hat{l}_{\\boldsymbol{x},\\boldsymbol{y}} = P(\\hat{y} = \\mathcal{C}_j|\\boldsymbol{x})\n",
    "$$\n",
    "多クラスロジスティック回帰の条件付確率は$\\boldsymbol{p}=\\sigma(\\boldsymbol{W}\\boldsymbol{x})$なので、\n",
    "$$\n",
    "\\hat{l}_{\\boldsymbol{x},\\boldsymbol{y}} = p_j = \\prod^{K}_{k=1}\\left\\{\n",
    "    \\begin{array}{l}\n",
    "        p_k \\quad (y_k = 1) \\\\\n",
    "        1 \\quad (y_k = 0)\n",
    "    \\end{array}\n",
    "    \\right . \n",
    "    = \\prod^{K}_{k=1}p_k^{y_k}\n",
    "$$\n",
    "またデータ$D$全体の尤度$\\hat{L}_D$は、\n",
    "$$\n",
    "\\hat{L}_D(\\boldsymbol{W}) = \\prod^{N}_{i=1}\\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W})\n",
    "$$\n",
    "これを目的関数とし、最大化させるような$\\boldsymbol{W}$を求める。また2値分類同様に目的関数を以下のように変更する。\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{\\hat{L}}_D(\\boldsymbol{W}) &= -log\\hat{L}_D(\\boldsymbol{W}) \\notag \\\\\n",
    "&= -\\sum^{N}_{i=i} log \\hat{l}_{\\boldsymbol{x}_i, \\boldsymbol{y}_i}(\\boldsymbol{W}) \\notag\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率的勾配降下法\n",
    "2値分類と同様に確率的勾配降下法によってパラメータを推定していく。\n",
    "$\\boldsymbol{W}$の更新式は\n",
    "$$\n",
    "\\boldsymbol{W}^{(t+1)} = \\boldsymbol{W}^{(t)} + \\eta_t \\nabla log\\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W}^{(t)})\n",
    "$$\n",
    "$\\boldsymbol{W}$に対しての偏微分はわかりづらいので$t$回目の反復で$\\boldsymbol{W}^{(t)}$の列ベクトル$\\boldsymbol{w}_j^{(t)}$毎に偏微分を計算しすべての$j$に対して重みベクトル$\\boldsymbol{w}_j^{(t)}$を更新する式に変更する。なお$j$は$\\boldsymbol{W}$の列番号である($1 \\leqq j \\leqq K$)。\n",
    "$$\n",
    "\\left .\n",
    "\\boldsymbol{w}_j^{(t+1)} = \\boldsymbol{w}_j^{(t)} + \\eta_t \\frac{\\partial log \\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_j} \\right|_{\\boldsymbol{W}=\\boldsymbol{W}^{(t)}}\n",
    "$$\n",
    "ここで$\\frac{\\partial log \\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_j}$について考える。\n",
    "まず、\n",
    "$$\n",
    "log \\hat{l}_{\\boldsymbol{x}_i, \\boldsymbol{y}_i} = log \\prod^{K}_{k=1}p_k^{y_k} = \\sum^{K}_{k=1} y_k log p_k \n",
    "$$\n",
    "なので\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial log \\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_i}\n",
    "&= \\frac{\\partial}{\\partial \\boldsymbol{w}_i}\\sum^{K}_{k=1} \\notag \\\\\n",
    "&= \\sum^{K}_{k=1}\\frac{y_k}{p_k} \\frac{\\partial p_k}{\\partial \\boldsymbol{w}_i} \\notag \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "また、\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial \\boldsymbol{w}_i} = \\frac{\\partial}{\\partial \\boldsymbol{w}_i}\\sigma(a)_k =\\frac{\\partial p_k}{\\partial a_i} \\frac{\\partial a_i}{\\partial \\boldsymbol{w}_i}\n",
    "$$\n",
    "ここで  \n",
    "(i)$k = i$のとき  \n",
    "$S = \\sum\\limits^{K}_{k=1}e^{x_k}$とおくと$p_k = \\frac{e^{x_k}}{S}$より\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial a_i} &= \\frac{e^{x_i}S - e^{x_i}e^{x_i}}{S^2} \\notag \\\\\n",
    "&= \\frac{e^{x_i}}{S} - (\\frac{e^{x_i}}{S}) ^ 2 \\notag \\\\\n",
    "&= y_i - y_i ^ 2 \\notag \\\\\n",
    "&= y_i(1 - y_i) \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "(ii)$k = i$のとき\n",
    "(i)と同様に\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_k}{\\partial a_i} &= - \\frac{e^{x_k} e^{x_i}}{S ^ 2} \\notag \\\\\n",
    "&= - \\frac{e^{x_k}}{S} \\frac{e^{x_i}}{S} \\notag \\\\\n",
    "&= - y_k y_i \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "(i)(ii)より、\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial a_i} \n",
    "= \\left \\{ \\begin{array}{l}\n",
    "    y_k(1 - y_k) \\;\\;\\; (k = i)\\\\\n",
    "    -y_k y_i \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (k \\neq i)\n",
    "\\end{array} \\right .\n",
    "$$\n",
    "また、\n",
    "$$\n",
    "\\delta_{ik} = \\left \\{ \\begin{array}{l}\n",
    "    1 \\;\\;\\; (k = i) \\\\\n",
    "    0 \\;\\;\\; (k \\neq i)\n",
    "\\end{array} \\right .\n",
    "$$\n",
    "を使用すると、\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial a_i} = y_k(\\delta_{ik} - y_i)\n",
    "$$\n",
    "となるので、\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial w_i} = y_k(\\delta_{ik} - y_i) \\boldsymbol{x}\n",
    "$$\n",
    "以上より、\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial log \\hat{l}_{\\boldsymbol{x}, \\boldsymbol{y}}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_j}\n",
    "&= \\sum^{K}_{k=1} \\frac{y_k}{p_k} p_k(\\delta_{ik} - p_i)\\boldsymbol{x} \\notag \\\\\n",
    "&= \\sum^{K}_{k=1} y_k(\\delta_{ik} - p_i) \\boldsymbol{x} \\notag \\\\\n",
    "&= (y_i - p_i \\sum^{K}_{k=1} p_k) \\boldsymbol{x} \\notag \\\\\n",
    "&= (y_i - p_i) \\boldsymbol{x} \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "したがって確率的勾配降下法による多クラスロジスティック回帰のパラメータ更新式は、\n",
    "$$\n",
    "\\boldsymbol{w}_j^{(t+1)} = \\boldsymbol{w}_j^{(t)} + \\eta_t(y_j - p_j^{(t)})\\boldsymbol{x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装\n",
    "多クラスロジスティック回帰を確率的勾配降下法にて実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import struct\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def read_image(fi):\n",
    "    magic, n, rows, columns = struct.unpack(\">IIII\", fi.read(16))\n",
    "    assert magic == 0x00000803\n",
    "    assert rows == 28\n",
    "    assert columns == 28\n",
    "    rawbuffer = fi.read()\n",
    "    assert len(rawbuffer) == n * rows * columns\n",
    "    rawdata = np.frombuffer(rawbuffer, dtype='>u1', count=n*rows*columns)\n",
    "    return rawdata.reshape(n, rows, columns).astype(np.float32) / 255.0\n",
    "\n",
    "def read_label(fi):\n",
    "    magic, n = struct.unpack(\">II\", fi.read(8))\n",
    "    assert magic == 0x00000801\n",
    "    rawbuffer = fi.read()\n",
    "    assert len(rawbuffer) == n\n",
    "    return np.frombuffer(rawbuffer, dtype='>u1', count=n)\n",
    "\n",
    "def openurl_gzip(url):\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        headers={\n",
    "            \"Accept-Encoding\": \"gzip\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11\", \n",
    "        })\n",
    "    response = urllib.request.urlopen(request)\n",
    "    return gzip.GzipFile(fileobj=response, mode='rb')\n",
    "\n",
    "def save_mnist():\n",
    "    if Path(\"data/mnist.npz\").exists():\n",
    "        return\n",
    "    np.savez_compressed(\n",
    "        \"data/mnist\",\n",
    "        train_x=read_image(openurl_gzip(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")),\n",
    "        train_y=read_label(openurl_gzip(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")),\n",
    "        test_x=read_image(openurl_gzip(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")),\n",
    "        test_y=read_label(openurl_gzip(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"))\n",
    "    )\n",
    "\n",
    "save_mnist()\n",
    "data = np.load(\"data/mnist.npz\")\n",
    "print(\"Training data (X):\", data[\"train_x\"].shape, data[\"train_x\"].dtype)\n",
    "print(\"Training data (Y):\", data[\"train_y\"].shape, data[\"train_y\"].dtype)\n",
    "print(\"Test data (X):\", data[\"test_x\"].shape, data[\"test_x\"].dtype)\n",
    "print(\"Test data (Y):\", data[\"test_y\"].shape, data[\"test_y\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def softmax(self, a):\n",
    "        # refer 6.5.1\n",
    "        ea = np.exp(a - np.max(a))\n",
    "        return ea / np.sum(ea)\n",
    "\n",
    "    def train(self, X, Y, num_class, eta=1e-3, alpha=1e-6, epoch=100000, eps=1e-6):\n",
    "        N = X.shape[0]\n",
    "        self.W = np.random.uniform(size=(X.shape[-1], num_class))\n",
    "        for t in range(epoch):\n",
    "            i = np.random.choice(N)\n",
    "            hat_y = self.predict_proba(X[i])\n",
    "            # to one-hot vector\n",
    "            y = np.zeros(num_class)\n",
    "            y[Y[i]] = 1.0\n",
    "            delta = (y - hat_y) * X[i].reshape((-1, 1)) - 2 * alpha * self.W / N\n",
    "            if np.sum(np.abs(delta)) < eps:\n",
    "                break\n",
    "            self.W += eta * delta\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        value = x @ self.W\n",
    "        if len(value.shape) < 2:\n",
    "            return self.softmax(value).flatten()\n",
    "        else:\n",
    "            return np.apply_along_axis(self.softmax, axis=1, arr=value)\n",
    "\n",
    "    def predict(self, x):\n",
    "        proba = self.predict_proba(x)\n",
    "        return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"train_x\"]\n",
    "\n",
    "\n",
    "def to_feature(X):\n",
    "    return np.c_[X.reshape((X.shape[0], -1)), np.ones(X.shape[0])]\n",
    "\n",
    "\n",
    "X_train = to_feature(X_train)\n",
    "model = LogisticClassifier().train(X=X_train, Y=data[\"train_y\"], num_class=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
